# Day 1-2: The 10,000-foot View

> **Goal**: Understand what makes a coding agent different from a chatbot, and see the high-level architecture of opencode.

---

## What Is a Coding Agent?

Let's start with what you probably already know: an LLM can generate code. You paste some code, ask a question, get an answer. That's a **chatbot**.

A **coding agent** is different. It's autonomous. You give it a task like "fix this bug" or "add a login feature" and it:

1. Reads files to understand the codebase
2. Decides what to do
3. Makes changes
4. Runs tests to verify
5. Repeats until done (or asks for help)

The key insight: **an agent is an LLM in a loop with access to tools**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚    CHATBOT                        AGENT                      â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€                      â”‚
â”‚                                                              â”‚
â”‚    User â”€â”€â–¶ LLM â”€â”€â–¶ Response      User â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                            â”‚   L O O P   â”‚  â”‚
â”‚    One shot.                               â”‚             â”‚  â”‚
â”‚    No memory.                              â”‚  LLM â—€â”€â”€â”   â”‚  â”‚
â”‚    No tools.                               â”‚   â”‚     â”‚   â”‚  â”‚
â”‚                                            â”‚   â–¼     â”‚   â”‚  â”‚
â”‚                                            â”‚ Tool â”€â”€â”€â”˜   â”‚  â”‚
â”‚                                            â”‚             â”‚  â”‚
â”‚                                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚         â”‚
â”‚                                                   â–¼         â”‚
â”‚                                              Done/Response  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Core Concept: Tool Calling

Before we dive into architecture, let's make sure "tool calling" is crystal clear.

When you call an LLM API, you can tell it: "Here are some tools you can use." The LLM doesn't actually run the tools - it just tells you which tool to call and with what arguments.

**Python example of what tool calling looks like:**

```python
# You send this to the LLM
messages = [
    {"role": "user", "content": "What files are in the src/ directory?"}
]

tools = [
    {
        "name": "list_files",
        "description": "List files in a directory",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Directory path"}
            }
        }
    }
]

response = client.chat(messages=messages, tools=tools)

# The LLM responds with something like:
# {
#     "tool_calls": [{
#         "name": "list_files",
#         "arguments": {"path": "src/"}
#     }]
# }

# YOUR CODE executes the tool, then sends the result back to the LLM
# The LLM can then respond to the user or call more tools
```

The LLM decides *what* tool to call. Your code *executes* it. This separation is fundamental.

---

## opencode's Architecture: The Bird's Eye View

Here's how opencode is structured at the highest level:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              opencode                                    â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    TUI      â”‚     â”‚              Core Engine                     â”‚    â”‚
â”‚  â”‚  (Terminal  â”‚â”€â”€â”€â”€â–¶â”‚                                              â”‚    â”‚
â”‚  â”‚    UI)      â”‚     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚ Session â”‚â”€â”€â–¶â”‚  Main   â”‚â”€â”€â–¶â”‚   LLM   â”‚   â”‚    â”‚
â”‚                      â”‚   â”‚ Manager â”‚   â”‚  Loop   â”‚   â”‚Interfaceâ”‚   â”‚    â”‚
â”‚                      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚                      â”‚                      â”‚                       â”‚    â”‚
â”‚                      â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚
â”‚                      â”‚              â–¼               â–¼               â”‚    â”‚
â”‚                      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚                      â”‚        â”‚   Tool   â”‚   â”‚Permissionâ”‚          â”‚    â”‚
â”‚                      â”‚        â”‚ Registry â”‚   â”‚  System  â”‚          â”‚    â”‚
â”‚                      â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚                      â”‚                                              â”‚    â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚  Supporting Systems:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Agent   â”‚ â”‚  Config  â”‚ â”‚  Skills  â”‚ â”‚   MCP    â”‚ â”‚  Event   â”‚      â”‚
â”‚  â”‚ Registry â”‚ â”‚  System  â”‚ â”‚  System  â”‚ â”‚ Servers  â”‚ â”‚   Bus    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Let's define each piece:

### Core Components

| Component | What It Does | In opencode |
|-----------|--------------|-------------|
| **Session Manager** | Stores conversation history, manages state | `session/index.ts` |
| **Main Loop** | The `while(true)` that orchestrates everything | `session/prompt.ts` |
| **LLM Interface** | Calls the AI model, handles streaming | `session/llm.ts` |
| **Tool Registry** | Registers and executes tools | `tool/registry.ts` |
| **Permission System** | Asks user before dangerous operations | `permission/next.ts` |

### Supporting Systems

| Component | What It Does | In opencode |
|-----------|--------------|-------------|
| **Agent Registry** | Different "modes" (explore, build, plan) | `agent/agent.ts` |
| **Config System** | User settings, model configuration | `config/config.ts` |
| **Skills System** | User-defined prompt templates | `skill/skill.ts` |
| **MCP Servers** | External tool providers | `mcp/index.ts` |
| **Event Bus** | Pub/sub for decoupled communication | `bus/index.ts` |

---

## The Main Loop: Heart of the Agent

The most important thing to understand is the main loop. Everything else supports it.

Here's the conceptual flow in Python-like pseudocode:

```python
def main_loop(session_id: str):
    """The heart of a coding agent."""

    while True:
        # 1. Get conversation history
        messages = get_messages(session_id)

        # 2. Find the last user message
        last_user = find_last_user_message(messages)
        last_assistant = find_last_assistant_message(messages)

        # 3. Should we continue?
        if should_stop(last_assistant):
            break

        # 4. Build the LLM request
        tools = get_available_tools()
        system_prompt = build_system_prompt()

        # 5. Call the LLM
        response = call_llm(
            messages=messages,
            tools=tools,
            system=system_prompt
        )

        # 6. Process the response
        if response.has_tool_calls:
            for tool_call in response.tool_calls:
                # Execute each tool
                result = execute_tool(tool_call)
                # Add result to conversation
                add_tool_result(session_id, tool_call, result)
            # Loop continues - LLM will see tool results
            continue

        # 7. LLM just responded with text - we're done (for this turn)
        save_response(session_id, response)
        break
```

This is the conceptual version. The actual opencode implementation in `session/prompt.ts` is more complex because it handles:

- Streaming responses (showing text as it arrives)
- Error recovery and retries
- Context overflow (compaction)
- Subagents (nested agent calls)
- Permission checks
- Multiple providers (Anthropic, OpenAI, etc.)

But the core idea is the same: **loop until the LLM stops calling tools**.

---

## When Does the Loop Stop?

This is a crucial question. The LLM can:

1. **Respond with text only** â†’ Stop (task complete or needs user input)
2. **Call tools** â†’ Continue (execute tools, send results back)
3. **Hit an error** â†’ Stop (with error message)
4. **Be cancelled by user** â†’ Stop

In opencode, this decision happens here (simplified):

```typescript
// From session/prompt.ts (simplified)
if (
    lastAssistant?.finish &&
    !["tool-calls", "unknown"].includes(lastAssistant.finish) &&
    lastUser.id < lastAssistant.id
) {
    // LLM finished without tool calls, exit loop
    break;
}
```

The `finish` field comes from the LLM API - it tells us *why* the model stopped generating:
- `"stop"` - Natural end of response
- `"tool-calls"` - Model wants to call tools
- `"length"` - Hit token limit
- `"content-filter"` - Content was filtered

---

## The Tool Lifecycle

When the LLM decides to call a tool, here's what happens:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Tool Lifecycle                            â”‚
â”‚                                                                  â”‚
â”‚   1. LLM Request                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ "Call the    â”‚                                           â”‚
â”‚      â”‚ read tool    â”‚                                           â”‚
â”‚      â”‚ with path    â”‚                                           â”‚
â”‚      â”‚ src/main.ts" â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚             â”‚                                                    â”‚
â”‚   2. Permission Check                                           â”‚
â”‚             â–¼                                                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ Is this      â”‚â”€â”€â–¶ Denied? â”€â”€â–¶ Return error to LLM       â”‚
â”‚      â”‚ allowed?     â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚             â”‚ Allowed                                            â”‚
â”‚             â–¼                                                    â”‚
â”‚   3. Tool Execution                                             â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ Read the     â”‚                                           â”‚
â”‚      â”‚ file from    â”‚                                           â”‚
â”‚      â”‚ disk         â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚             â”‚                                                    â”‚
â”‚   4. Result Formatting                                          â”‚
â”‚             â–¼                                                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ Format as    â”‚                                           â”‚
â”‚      â”‚ tool result  â”‚                                           â”‚
â”‚      â”‚ message      â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚             â”‚                                                    â”‚
â”‚   5. Back to LLM                                                â”‚
â”‚             â–¼                                                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ Add to       â”‚                                           â”‚
â”‚      â”‚ conversation,â”‚                                           â”‚
â”‚      â”‚ continue     â”‚                                           â”‚
â”‚      â”‚ loop         â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Insight: The Prompt Is Everything

The system prompt is what makes a coding agent behave like a coding agent. Without it, the LLM would just be a generic assistant.

opencode's system prompt (in `session/prompt/`) tells the LLM:

1. **What it is**: "You are a coding assistant..."
2. **What tools it has**: Descriptions of each tool
3. **How to behave**: When to use tools, when to ask questions
4. **Constraints**: Don't delete files without asking, etc.

Here's a tiny excerpt from opencode's Anthropic prompt:

```
You are an interactive CLI tool that helps users with software engineering tasks.
Use the instructions below and the tools available to you to assist the user.

# Doing tasks
The user will primarily request you perform software engineering tasks...
For these tasks the following steps are recommended:
1. Use the TodoWrite tool to plan the task if required
2. Implement the solution using available tools
3. Verify the solution works
```

The prompt is hundreds of lines. It's the "personality" and "knowledge" that makes the agent effective.

---

## What Makes opencode Different?

opencode isn't just a basic agent loop. It has several sophisticated features:

### 1. Multiple Agent Modes

Different tasks need different capabilities:

| Agent | Purpose | Tools Available |
|-------|---------|-----------------|
| `build` | Default mode, full capabilities | All tools |
| `explore` | Read-only exploration | Only read/search tools |
| `plan` | Planning mode | Limited tools, focused on thinking |
| `compaction` | Internal: summarize long conversations | No tools |

### 2. Permission System

Before executing certain tools, opencode asks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ opencode wants to run:                      â”‚
â”‚                                             â”‚
â”‚   bash: rm -rf node_modules                 â”‚
â”‚                                             â”‚
â”‚ [Allow] [Allow Always] [Deny]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The permission system uses rules like:
- `bash`: ask (always ask user)
- `read`: allow (safe operation)
- `read *.env`: ask (sensitive files)

### 3. Context Management (Compaction)

Conversations can get long. LLMs have context limits. opencode handles this with "compaction":

1. Detect when context is getting full
2. Use a separate LLM call to summarize the conversation
3. Replace old messages with the summary
4. Continue with room to spare

### 4. MCP (Model Context Protocol)

MCP lets you add external tools without modifying opencode:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  opencode   â”‚â—€â”€â”€â”€â”€â”€â”€â–¶â”‚ MCP Server  â”‚
â”‚             â”‚  JSON  â”‚ (e.g., DB   â”‚
â”‚             â”‚  RPC   â”‚  access)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

You can write your own MCP servers to give the agent new capabilities.

---

## Looking at the Code: Where Things Live

Let's map concepts to files:

```
packages/opencode/src/
â”‚
â”œâ”€â”€ session/
â”‚   â”œâ”€â”€ prompt.ts        â† THE MAIN LOOP (start here!)
â”‚   â”œâ”€â”€ processor.ts     â† Handles streaming, tool results
â”‚   â”œâ”€â”€ llm.ts           â† Calls the LLM API
â”‚   â”œâ”€â”€ message-v2.ts    â† Message format and storage
â”‚   â”œâ”€â”€ compaction.ts    â† Summarizes long conversations
â”‚   â””â”€â”€ prompt/          â† System prompt templates
â”‚       â”œâ”€â”€ anthropic.txt
â”‚       â”œâ”€â”€ codex.txt
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ tool/
â”‚   â”œâ”€â”€ tool.ts          â† Tool interface definition
â”‚   â”œâ”€â”€ registry.ts      â† Tool registration
â”‚   â”œâ”€â”€ read.ts          â† Read tool implementation
â”‚   â”œâ”€â”€ bash.ts          â† Bash tool implementation
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ agent.ts         â† Agent definitions (build, explore, etc.)
â”‚
â”œâ”€â”€ permission/
â”‚   â””â”€â”€ next.ts          â† Permission checking logic
â”‚
â”œâ”€â”€ skill/
â”‚   â””â”€â”€ skill.ts         â† User-defined prompt templates
â”‚
â”œâ”€â”€ mcp/
â”‚   â””â”€â”€ index.ts         â† MCP server connections
â”‚
â””â”€â”€ config/
    â””â”€â”€ config.ts        â† Configuration management
```

---

## Summary: The Mental Model

Here's your mental model for understanding coding agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CODING AGENT = 4 THINGS                   â”‚
â”‚                                                              â”‚
â”‚   1. A LOOP         while (not done):                        â”‚
â”‚                         response = llm(messages, tools)      â”‚
â”‚                         if response.has_tools:               â”‚
â”‚                             execute_tools(response.tools)    â”‚
â”‚                                                              â”‚
â”‚   2. TOOLS          Things the LLM can do:                   â”‚
â”‚                     read files, edit files, run commands     â”‚
â”‚                                                              â”‚
â”‚   3. MEMORY         Conversation history that persists       â”‚
â”‚                     across loop iterations                   â”‚
â”‚                                                              â”‚
â”‚   4. PROMPT         Instructions that tell the LLM           â”‚
â”‚                     how to behave                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Everything else - permissions, agents, compaction, MCP - are refinements of these core ideas.

---

## What's Next?

In **Day 3-4**, we'll dive deep into the main loop, walking through `session/prompt.ts` line by line.

But first, let's cement this understanding with an assignment.

---

## Assignment: Build the Simplest Possible Agent

**Goal**: Create a minimal Python agent that demonstrates the core loop.

Your agent should:
1. Accept user input
2. Call an LLM with tools available
3. Execute tools if requested
4. Loop until the LLM responds without tool calls

**Start here**: [Assignment 1: Minimal Agent Loop](./assignments/01-minimal-agent.md)

---

## Optional Deep Dives

Want to understand the architecture in more detail?

- ğŸ“– [Architecture Overview](/deep-dives/overview.md) - Codebase structure, design patterns, event flow
- ğŸ—ï¸ [Event Bus](/deep-dives/event-bus.md) - How the engine decouples from the UI
- ğŸ”§ [Provider Abstraction](/deep-dives/provider-abstraction.md) - Multi-LLM support
